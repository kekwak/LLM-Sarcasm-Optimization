# Sarcasm Detection: DSPy vs Fine-Tuning

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å–∞—Ä–∫–∞–∑–º–∞ –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö.
–í —Ä–∞–±–æ—Ç–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ **Fine-Tuning (ModernBERT)** —Å –Ω–æ–≤–µ–π—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ **Automated Prompt Optimization (DSPy)** –Ω–∞ –±–∞–∑–µ LLM.

### üèÜ –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–µ—Ç–æ–¥ | –ú–æ–¥–µ–ª—å | Accuracy | F1 Score | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
| :--- | :--- | :--- | :--- | :--- |
| **MIPROv2 (DSPy)** | Gemma-3-27B (4-bit) | **85.25%** | **0.8499** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ (Bootstrap) |
| **Manual Prompt** | Gemma-3-27B (4-bit) | 81.25% | 0.8256 | –†—É—á–Ω–æ–π Few-Shot + Chain-of-Thought |
| **GEPA (DSPy)** | Gemma-3-27B (4-bit) | 76.12% | 0.7931 | –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π |
| **Fine-Tuning** | ModernBERT-large | 75.88% | 0.7360 | –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ encoder-–º–æ–¥–µ–ª–∏ |

**–í—ã–≤–æ–¥:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö LLM —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º MIPROv2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Fine-Tuning –Ω–∞ **~10%** –∏ —Ä—É—á–Ω–æ–π –∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –Ω–∞ **~4%** –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (200 training samples).

### üõ† –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

*   **Inference:** vLLM (Docker) + bitsandbytes (4-bit quantization)
*   **Frameworks:** DSPy, PyTorch, HuggingFace Transformers
*   **Models:**
    - `unsloth/gemma-3-27b-it-bnb-4bit`
    - `unsloth/gemma-3-12b-it-bnb-4bit`
    - `unsloth/Qwen3-14B-bnb-4bit`
    - `openai/gpt-oss-20b`
    - `answerdotai/ModernBERT-large`
    - `FacebookAI/xlm-roberta-large`

### üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

*   `main.ipynb` ‚Äî **–û—Å–Ω–æ–≤–Ω–æ–π –Ω–æ—É—Ç–±—É–∫** —Å–æ –≤—Å–µ–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º.
*   `artifacts/` ‚Äî –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã:
    *   `sarcasm_typed_optimized_85.json` ‚Äî –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (MIPROv2).
    *   `sarcasm_gepa_optimized_76.json` ‚Äî –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
*   `requirements.txt` ‚Äî –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python.

### üöÄ –ó–∞–ø—É—Å–∫

1.  **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:**
    ```nushell
    pip install -r requirements.txt
    ```

2.  **–ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞ (Docker):**
    ```nushell
    docker run -d \
      --gpus all \
      --ipc=host \
      -p 8000:8000 \
      -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
      -v ~/.cache/huggingface:/root/.cache/huggingface \
      vllm/vllm-openai:latest \
      --model unsloth/gemma-3-27b-it-bnb-4bit \
      --gpu-memory-utilization 0.92 \
      --max-model-len 16384 \
      -enforce-eager
    ```

3.  **–ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:**
    –û—Ç–∫—Ä–æ–π—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ `main.ipynb`.

### üìâ –ù–µ—É–¥–∞–≤—à–∏–µ—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
–í —Ö–æ–¥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–∞–∫–∂–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –º–æ–¥–µ–ª–∏ **gpt-oss-20B**, **Qwen3-14B** –∏ **gemma3-12b**. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 1-—è –∏ 2-—è –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–º–µ—Ç–Ω–æ –Ω–∏–∂–µ 3-–π (–≤ —Ä–∞–π–æ–Ω–µ 71% –∏ 77% Accuracy –ø—Ä–æ—Ç–∏–≤ 80%).

–ú–æ–≥—É —Å–≤—è–∑–∞—Ç—å —ç—Ç–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (–≤ —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MoE ‚Äî –º–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ **–∞–∫—Ç–∏–≤–Ω—ã—Ö** –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω) –∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

### üìå –í—ã–≤–æ–¥
MIPROv2 (DSPy) —Å Gemma-3-27B –¥–∞–ª –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –º–æ–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö: 85.25% Accuracy / 0.8499 F1, —á—Ç–æ –Ω–∞ +9.37 p.p. –≤—ã—à–µ Fine-Tuning ModernBERT-large –∏ –Ω–∞ +4.00 p.p. –≤—ã—à–µ —Ä—É—á–Ω–æ–≥–æ few-shot –ø—Ä–æ–º–ø—Ç–∞ –ø—Ä–∏ 200 train samples.
